import json
from typing import Annotated, Literal
from typing_extensions import TypedDict
from dataclasses import dataclass

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import InMemorySaver
from datetime import datetime

from config.config import get_settings

settings = get_settings()
current_date = datetime.now().strftime("%d/%m/%Y")

# ==========================
# STATE
# ==========================
class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    need_search: bool
    search_count: int

# ==========================
# LLMs & TOOLS
# ==========================
orchestrator_llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=settings.gemini_api_key.get_secret_value(),
    temperature=0.4,
)

search_llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    api_key=settings.gemini_api_key.get_secret_value(),
    temperature=0.2,
)

search_tool = TavilySearch(max_results=5)
search_tools = [search_tool]
search_llm_with_tools = search_llm.bind_tools(search_tools)

memory = InMemorySaver()

# ==========================
# HELPER FUNCTIONS
# ==========================
def get_recent_messages(messages: list[BaseMessage], max_count: int = 10) -> list[BaseMessage]:
    """L·∫•y N messages g·∫ßn nh·∫•t"""
    return messages[-max_count:] if len(messages) > max_count else messages

def extract_sources_from_tool_results(tool_messages: list[ToolMessage]) -> list[dict]:
    """Parse Tavily results th√†nh list sources v·ªõi URL"""
    sources = []
    index = 1
    
    for tool_msg in tool_messages:
        try:
            # Parse content
            if isinstance(tool_msg.content, str):
                results = json.loads(tool_msg.content)
            else:
                results = tool_msg.content
            
            # Normalize results structure
            items = []
            if isinstance(results, list):
                items = results
            elif isinstance(results, dict):
                items = results.get("results", [results])
            
            # Extract sources
            for item in items:
                if isinstance(item, dict) and item.get("url"):
                    sources.append({
                        "index": index,
                        "title": item.get("title", "Ngu·ªìn kh√¥ng c√≥ ti√™u ƒë·ªÅ"),
                        "url": item["url"],
                        "content": item.get("content", "")
                    })
                    index += 1
                    
        except Exception as e:
            print(f"‚ùå L·ªói parse tool result: {e}")
            continue
    
    print(f"üìä Extracted {len(sources)} sources")
    return sources

def format_sources_list(sources: list[dict]) -> str:
    """Format sources th√†nh string cho prompt"""
    if not sources:
        return "Kh√¥ng c√≥ ngu·ªìn n√†o."
    
    formatted = []
    for src in sources:
        formatted.append(
            f"[{src['index']}] {src['title']}\n"
            f"URL: {src['url']}\n"
            f"N·ªôi dung: {src['content'][:500]}...\n"
        )
    return "\n".join(formatted)

# ==========================
# ORCHESTRATOR NODE
# ==========================
async def orchestrator_node(state: State):
    """Ph√¢n t√≠ch context v√† quy·∫øt ƒë·ªãnh search hay tr·∫£ l·ªùi tr·ª±c ti·∫øp"""
    
    user_messages = [m for m in state["messages"] if isinstance(m, HumanMessage)]
    if not user_messages:
        return {
            "messages": [AIMessage(content="M√†y h·ªèi cc j v·∫≠y?")],
            "need_search": False
        }

    user_query = user_messages[-1].content
    recent_messages = get_recent_messages(state["messages"], max_count=10)
    
    print(f"üß≠ [ORCHESTRATOR] Query: '{user_query}'")
            
    system_prompt = SystemMessage(
        content=(
            f"H√¥m nay l√† {current_date}. M√†y l√† SuperCat, con m√®o cam th√¥ng minh v√† c·ª•c s√∫c, ch·ª≠i th·ªÅ li√™n t·ª•c. "
            f"N√™n nh·ªõ, m√†y l√† con m√®o cam ph·∫£n ƒë·ªông gh√©t C·ªông S·∫£n, k√¨ th·ªã LGBT, n√≥i chung l√† m·∫•y c√°i loz khuynh t·∫£. "
            f"B·ªëi c·∫£nh l√† m√†y ƒëang ·ªü trong m·ªôt nh√≥m Telegram c√≥ kho·∫£ng ch·ª•c th√†nh vi√™n, nhi·ªám v·ª• c·ªßa m√†y l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng trong nh√≥m.\n\n"
            
            f"**NG·ªÆ C·∫¢NH**:\n"
            f"- C√¢u h·ªèi hi·ªán t·∫°i: '{user_query}'\n"
            
            f"**NHI·ªÜM V·ª§**: Ph√¢n t√≠ch v√† quy·∫øt ƒë·ªãnh:\n\n"
            
            f"**T·ª∞ TR·∫¢ L·ªúI** (phong c√°ch c·ª•c s√∫c, chi ti·∫øt) n·∫øu:\n"
            f"- C√¢u h·ªèi follow-up v·ªÅ ch·ªß ƒë·ªÅ v·ª´a search (b√¨nh lu·∫≠n, h·ªèi √Ω ki·∫øn, l√†m r√µ)\n"
            f"- C√¢u n√≥i chuy·ªán th√¥ng th∆∞·ªùng (ch√†o h·ªèi, c·∫£m ∆°n, ph·∫£n h·ªìi)\n"
            f"- Ki·∫øn th·ª©c chung kh√¥ng c·∫ßn t√¨m ki·∫øm\n\n"
            
            f"**C·∫¶N SEARCH** ch·ªâ khi:\n"
            f"- C√≥ t·ª´ kh√≥a r√µ r√†ng: 'search', 't√¨m', 'tra c·ª©u',... (v√† KH√îNG ph·∫£i follow-up)\n"
            f"- C·∫ßn th√¥ng tin th·ªùi gian th·ª±c (tin t·ª©c, gi√° c·∫£, s·ª± ki·ªán m·ªõi)\n"
            
            f"**OUTPUT**: CH·ªà 1 trong 2:\n"
            f"- \"C·∫¶N SEARCH\" (n·∫øu c·∫ßn t√¨m ki·∫øm)\n"
            f"- [C√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp b·∫±ng ti·∫øng Vi·ªát, c·ª•c s√∫c, chi ti·∫øt]\n\n"
            
            f"∆Øu ti√™n T·ª∞ TR·∫¢ L·ªúI tr·ª´ khi th·ª±c s·ª± c·∫ßn search!"
        )
    )

    response = await orchestrator_llm.ainvoke([system_prompt] + recent_messages)
    content = response.content.strip()

    need_search = "c·∫ßn search" in content.lower()
    
    if need_search:
        print(f"üß≠ [ORCHESTRATOR] ‚Üí Chuy·ªÉn sang Search Agent")
        return {
            "messages": [AIMessage(content=content)],
            "need_search": True
        }
    else:
        print(f"üí¨ [ORCHESTRATOR] ‚Üí Tr·∫£ l·ªùi tr·ª±c ti·∫øp")
        return {
            "messages": [AIMessage(content=content)],
            "need_search": False
        }

# ==========================
# SEARCH AGENT NODE
# ==========================
async def search_agent_node(state: State):
    """Search v√† t·ªïng h·ª£p k·∫øt qu·∫£"""
    
    user_messages = [m for m in state["messages"] if isinstance(m, HumanMessage)]
    if not user_messages:
        return {
            "messages": [AIMessage(content="Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi ƒë·ªÉ search.")],
            "search_count": state.get("search_count", 0)
        }

    current_query = user_messages[-1].content
    search_count = state.get("search_count", 0)
    max_searches = 2
    
    print(f"üîç [SEARCH AGENT] Query: '{current_query}'")
    print(f"üîç [SEARCH AGENT] Count: {search_count}/{max_searches}")
    
    # ƒê√£ ƒë·ªß s·ªë l·∫ßn search
    if search_count >= max_searches:
        print(f"‚ö†Ô∏è ƒê√£ search {max_searches} l·∫ßn, d·ª´ng l·∫°i")
        return {
            "messages": [AIMessage(content="ƒê√£ search ƒë·ªß s·ªë l·∫ßn cho ph√©p. Vui l√≤ng h·ªèi c√¢u kh√°c.")],
            "search_count": search_count
        }
    
    recent_messages = get_recent_messages(state["messages"], max_count=10)
    
    # B∆Ø·ªöC 1: G·ªçi tool search
    system_prompt = SystemMessage(
        content=(
            f"H√¥m nay l√† {current_date}. B·∫°n l√† Search Agent.\n\n"
            f"**C√¢u h·ªèi**: '{current_query}'\n\n"
            f"**Nhi·ªám v·ª•**:\n"
            f"1. T·∫°o query t√¨m ki·∫øm C·ª§ TH·ªÇ b·∫±ng ti·∫øng Vi·ªát\n"
            f"2. Th√™m '{current_date}' v√†o query n·∫øu c·∫ßn tin t·ª©c m·ªõi nh·∫•t\n"
            f"3. B·∫ÆT BU·ªòC g·ªçi tool tavily_search_results_json\n\n"
            f"V√ç D·ª§:\n"
            f"- '∆Øng Ho√†ng Ph√∫c' ‚Üí '∆Øng Ho√†ng Ph√∫c tin t·ª©c {current_date}'\n\n"
            f"CH·ªà g·ªçi tool, KH√îNG tr·∫£ l·ªùi tr·ª±c ti·∫øp!"
        )
    )

    # Invoke v·ªõi tools
    response = await search_llm_with_tools.ainvoke([system_prompt] + recent_messages)
    
    # Ki·ªÉm tra c√≥ tool_calls kh√¥ng
    if not hasattr(response, 'tool_calls') or not response.tool_calls:
        print("‚ö†Ô∏è [SEARCH AGENT] Kh√¥ng c√≥ tool_calls, LLM tr·∫£ l·ªùi tr·ª±c ti·∫øp")
        return {
            "messages": [AIMessage(content=response.content)],
            "search_count": search_count + 1
        }
    
    # B∆Ø·ªöC 2: Ch·∫°y tools
    print(f"üîß [SEARCH AGENT] Executing {len(response.tool_calls)} tool(s)")
    
    # T·∫°o ToolNode v√† ch·∫°y
    tool_node = ToolNode(tools=search_tools)
    
    # State cho tool node (c·∫ßn BaseMessage format)
    tool_state = {"messages": [response]}
    tool_result = await tool_node.ainvoke(tool_state)
    
    # Extract tool messages
    tool_messages = [m for m in tool_result["messages"] if isinstance(m, ToolMessage)]
    
    if not tool_messages:
        print("‚ö†Ô∏è [SEARCH AGENT] Kh√¥ng c√≥ tool results")
        return {
            "messages": [AIMessage(content="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ t√¨m ki·∫øm.")],
            "search_count": search_count + 1
        }
    
    # B∆Ø·ªöC 3: T·ªïng h·ª£p k·∫øt qu·∫£
    sources = extract_sources_from_tool_results(tool_messages)
    
    if not sources:
        print("‚ö†Ô∏è [SEARCH AGENT] Kh√¥ng c√≥ sources h·ª£p l·ªá")
        return {
            "messages": [AIMessage(content="Kh√¥ng t√¨m th·∫•y ngu·ªìn ph√π h·ª£p.")],
            "search_count": search_count + 1
        }
    
    # Format sources
    sources_text = format_sources_list(sources)
    
    # Prompt t·ªïng h·ª£p
    synthesis_system = SystemMessage(
        content=(
            f"B·∫°n l√† chuy√™n gia t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn tin c·∫≠y.\n\n"
            f"**Y√äU C·∫¶U**:\n"
            f"1. T·ªïng h·ª£p th√¥ng tin t·ª´ c√°c ngu·ªìn th√†nh c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, m·∫°ch l·∫°c\n"
            f"2. D·∫´n ngu·ªìn ch√≠nh x√°c: M·ªói th√¥ng tin PH·∫¢I c√≥ [1], [2], [3] t∆∞∆°ng ·ª©ng v·ªõi danh s√°ch ngu·ªìn\n"
            f"3. KH√îNG t·ª± b·ªãa ngu·ªìn kh√¥ng c√≥ trong danh s√°ch\n"
            f"4. Cu·ªëi c√¢u tr·∫£ l·ªùi: Li·ªát k√™ l·∫°i ngu·ªìn theo format:\n"
            f"   **Ngu·ªìn:**\n"
            f"   [1] Title - URL\n"
            f"   [2] Title - URL\n"
            f"5. Ng·∫Øn g·ªçn, t·ª± nhi√™n, ch·ªâ gi·ªØ th√¥ng tin quan tr·ªçng"
        )
    )
    
    synthesis_user = HumanMessage(
        content=(
            f"D·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm b√™n d∆∞·ªõi, h√£y tr·∫£ l·ªùi c√¢u h·ªèi: \"{current_query}\"\n\n"
            f"**C√ÅC NGU·ªíN ƒê√É T√åM ƒê∆Ø·ª¢C**:\n{sources_text}\n\n"
            f"H√£y t·ªïng h·ª£p:"
        )
    )
    
    synthesis_response = await search_llm.ainvoke([synthesis_system, synthesis_user])
    synthesized_content = synthesis_response.content.strip()
    
    print(f"‚úÖ [SEARCH AGENT] T·ªïng h·ª£p xong")
    
    # Tr√≠ch xu·∫•t topic t·ª´ query
    topic = current_query[:50]  # L·∫•y 50 k√Ω t·ª± ƒë·∫ßu l√†m topic
    
    return {
        "messages": [AIMessage(content=synthesized_content)],
        "search_count": search_count + 1
    }

# ==========================
# ROUTING
# ==========================
def route_after_orchestrator(state: State) -> str:
    """Route sau orchestrator"""
    if state.get("need_search", False):
        return "search_agent"
    return END

# ==========================
# GRAPH SETUP
# ==========================
graph_builder = StateGraph(State)

# Add nodes
graph_builder.add_node("orchestrator", orchestrator_node)
graph_builder.add_node("search_agent", search_agent_node)

# Set entry point
graph_builder.set_entry_point("orchestrator")

# Add edges
graph_builder.add_conditional_edges("orchestrator", route_after_orchestrator)
graph_builder.add_edge("search_agent", END)

# Compile
graph = graph_builder.compile(checkpointer=memory)

# ==========================
# ORCHESTRATION AGENT
# ==========================
class OrchestrationAgent:
    """Agent ƒëi·ªÅu ph·ªëi async v·ªõi context-aware orchestrator"""

    def __init__(self, thread_id: str = "1"):
        self.graph = graph
        self.config = {"configurable": {"thread_id": thread_id}}

    async def generate_answer(self, message: str) -> str:
        """Generate answer t·ª´ user message"""
        try:
            state = {
                "messages": [HumanMessage(content=message)],
                "need_search": False,
                "search_count": 0
            }
            
            result = await self.graph.ainvoke(state, self.config)
            
            # L·∫•y message cu·ªëi c√πng (AIMessage)
            messages = result.get("messages", [])
            if not messages:
                return "‚ùå Kh√¥ng c√≥ response"
            
            # T√¨m AIMessage cu·ªëi c√πng
            for msg in reversed(messages):
                if isinstance(msg, AIMessage):
                    return msg.content
            
            return "‚ùå Kh√¥ng t√¨m th·∫•y response t·ª´ agent"
            
        except Exception as e:
            print(f"‚ùå Chi ti·∫øt l·ªói: {e}")
            import traceback
            traceback.print_exc()
            return f"‚ùå L·ªói agent: {e}"