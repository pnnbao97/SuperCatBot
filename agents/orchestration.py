import asyncio
from typing import Annotated
from typing_extensions import TypedDict

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import InMemorySaver

from config.config import get_settings

settings = get_settings()

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]
    need_search: bool

# ==========================
# ‚ö° LLMs
# ==========================
orchestrator_llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    api_key=settings.gemini_api_key.get_secret_value(),
    temperature=0.4,
)

search_llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    api_key=settings.gemini_api_key.get_secret_value(),
    temperature=0.2,
)

search_tool = TavilySearch(max_results=3)
search_tools = [search_tool]
search_llm_with_tools = search_llm.bind_tools(search_tools)

# ==========================
# ‚ö° Memory
# ==========================
class CleanMemorySaver(InMemorySaver):
    def __init__(self, max_messages: int = 50):
        super().__init__()
        self.max_messages = max_messages

    def put(self, config, state, *args, **kwargs):
        try:
            msgs = state.get("messages", [])
            if isinstance(msgs, list):
                filtered = [m for m in msgs if isinstance(m, (HumanMessage, AIMessage))]
                state["messages"] = filtered[-self.max_messages:]
        except Exception as e:
            print(f"[CleanMemorySaver] warning: {e}")
        return super().put(config, state, *args, **kwargs)

memory = CleanMemorySaver(max_messages=50)

def clean_chat_history(messages, max_messages=6):
    """
    Tr√≠ch xu·∫•t l·ªãch s·ª≠ chat g·ªçn g√†ng cho LLM.
    Gi·ªØ t·ªëi ƒëa `max_messages` tin nh·∫Øn g·∫ßn ƒë√¢y.
    Lo·∫°i b·ªè metadata th·ª´a, ch·ªâ gi·ªØ n·ªôi dung.
    """
    recent = messages[-max_messages:]
    cleaned = []
    for m in recent:
        if isinstance(m, HumanMessage):
            cleaned.append(HumanMessage(content=m.content))
        elif isinstance(m, AIMessage):
            cleaned.append(AIMessage(content=m.content))
        elif hasattr(m, "type") and m.type == "tool":
            # Chuy·ªÉn ToolMessage th√†nh AIMessage t√≥m t·∫Øt n·ªôi dung tool
            summary = f"[Tool {getattr(m, 'name', 'unknown')}] {str(m.content)[:500]}"
            cleaned.append(AIMessage(content=summary))
        # B·ªè qua c√°c lo·∫°i message kh√°c ho·∫∑c unknown
    return cleaned

# ==========================
# ‚ö° Helper: logic search
# ==========================
def should_search(query: str) -> bool:
    query_lower = query.lower()
    explicit_keywords = ["search", "t√¨m ki·∫øm", "tra c·ª©u", "google", "t√¨m gi√∫p", "ki·∫øm gi√∫p", "tra gi√∫p", "t√¨m th√¥ng tin", "t√¨m tin"]
    time_sensitive_keywords = ["tin t·ª©c", "tin m·ªõi", "m·ªõi nh·∫•t", "v·ª´a x·∫£y ra", "h√¥m nay", "h√¥m qua", "tu·∫ßn n√†y", "th√°ng n√†y", "hi·ªán t·∫°i", "b√¢y gi·ªù", "l√∫c n√†y"]
    general_knowledge = ["l√† g√¨", "ƒë·ªãnh nghƒ©a", "gi·∫£i th√≠ch", "c√°ch th·ª©c", "t·∫°i sao", "l√†m th·∫ø n√†o", "v√≠ d·ª• v·ªÅ", "so s√°nh", "kh√°c nhau", "gi·ªëng nhau"]

    if any(kw in query_lower for kw in explicit_keywords):
        return True
    if any(kw in query_lower for kw in general_knowledge):
        return False
    if any(kw in query_lower for kw in time_sensitive_keywords):
        return True
    return False

# ==========================
# ‚ö° Async nodes
# ==========================
async def orchestrator_node(state: State):
    user_messages = [m for m in state["messages"] if isinstance(m, HumanMessage)]
    if not user_messages:
        return {"messages": [AIMessage(content="Xin l·ªói, t√¥i kh√¥ng nh·∫≠n ƒë∆∞·ª£c c√¢u h·ªèi.")]}

    user_query = user_messages[-1].content

    if should_search(user_query):
        print(f"üß≠ [KEYWORD MATCH] ƒêi·ªÅu ph·ªëi ‚Üí Search Agent v·ªõi query: {user_query}")
        return {"messages": [AIMessage(content="ƒêang t√¨m ki·∫øm th√¥ng tin...")], "need_search": True}

    recent_messages = clean_chat_history(state["messages"], max_messages=6)
    print(f"üß≠ [CONTEXT] ƒêi·ªÅu ph·ªëi ‚Üí Context: {recent_messages}")

    system_prompt = SystemMessage(
        content=(
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch. ƒê·ªçc l·ªãch s·ª≠ h·ªôi tho·∫°i ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh.\n\n"
            "CH·ªà tr·∫£ l·ªùi 'C·∫¶N SEARCH' n·∫øu c√¢u h·ªèi TH·ª∞C S·ª∞ c·∫ßn d·ªØ li·ªáu th·ªùi gian th·ª±c, "
            "v√≠ d·ª•: gi√° c·ªï phi·∫øu hi·ªán t·∫°i, k·∫øt qu·∫£ b√≥ng ƒë√° h√¥m nay, tai n·∫°n m·ªõi x·∫£y ra.\n\n"
            "V·ªöI T·∫§T C·∫¢ c√°c c√¢u h·ªèi kh√°c, h√£y tr·∫£ l·ªùi tr·ª±c ti·∫øp b·∫±ng ti·∫øng Vi·ªát.\n"
            "CH·ªà tr·∫£ l·ªùi 'C·∫¶N SEARCH' ho·∫∑c tr·∫£ l·ªùi c√¢u h·ªèi tr·ª±c ti·∫øp."
        )
    )

    response = await orchestrator_llm.ainvoke([system_prompt] + recent_messages)
    content = response.content.strip()

    if "c·∫ßn search" in content.lower():
        print(f"üß≠ [LLM CONFIRM] ƒêi·ªÅu ph·ªëi ‚Üí Search Agent v·ªõi query: {user_query}")
        return {"messages": [AIMessage(content="ƒêang t√¨m ki·∫øm th√¥ng tin...")], "need_search": True}

    print(f"üí¨ ƒêi·ªÅu ph·ªëi ‚Üí Tr·∫£ l·ªùi tr·ª±c ti·∫øp: {content[:60]}...")
    return {"messages": [response], "need_search": False}


async def search_agent_node(state: State):
    user_messages = [m for m in state["messages"] if isinstance(m, HumanMessage)]
    if not user_messages:
        return {"messages": [AIMessage(content="Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi ƒë·ªÉ t√¨m ki·∫øm.")]}

    recent_messages = clean_chat_history(state["messages"], max_messages=6)

    system_prompt = SystemMessage(
        content=(
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω chuy√™n t√¨m ki·∫øm th√¥ng tin th·ªùi gian th·ª±c T·∫†I VI·ªÜT NAM.\n"
            "T·∫°o query t√¨m ki·∫øm ƒë·∫ßy ƒë·ªß v√† c·ª• th·ªÉ, s·ª≠ d·ª•ng c√¥ng c·ª• tavily_search_results_json.\n"
            "T√≥m t·∫Øt k·∫øt qu·∫£ b·∫±ng ti·∫øng Vi·ªát."
        )
    )

    response = await search_llm_with_tools.ainvoke([system_prompt] + recent_messages)
    print(f"üîç Search Agent response: {response}")
    return {"messages": [response]}


async def process_tool_results(state: State):
    """Process tool results"""
    print("üß≠ [PROCESS TOOL RESULTS] ƒêang x·ª≠ l√Ω k·∫øt qu·∫£ t·ª´ tools...")
    
    # L·∫•y tool messages
    tool_messages = [m for m in state["messages"] if hasattr(m, 'type') and m.type == 'tool']
    
    if not tool_messages:
        return {"messages": [AIMessage(content="Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ t√¨m ki·∫øm.")]}
    
    # L·∫•y user query ban ƒë·∫ßu
    user_messages = [m for m in state["messages"] if isinstance(m, HumanMessage)]
    original_query = user_messages[-1].content if user_messages else ""
    
    # T·ªïng h·ª£p k·∫øt qu·∫£
    system_prompt = SystemMessage(
        content=(
            f"D·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm b√™n d∆∞·ªõi, h√£y tr·∫£ l·ªùi c√¢u h·ªèi: '{original_query}' "
            "nh∆∞ m·ªôt chuy√™n gia t·ªïng h·ª£p v√† ph√¢n t√≠ch th√¥ng tin, tr√≠ch d·∫´n ngu·ªìn n·∫øu c√≥ th·ªÉ (ngu·ªìn l√† ƒë∆∞·ªùng link url)."
        )
    )
    
    final_response = await search_llm.ainvoke([system_prompt] + state["messages"][-5:])
    
    print(f"üí¨ C√¢u tr·∫£ l·ªùi cu·ªëi: {final_response}")
    return {"messages": [final_response]}
# ==========================
# ‚ö° Graph setup
# ==========================
graph_builder = StateGraph(State)
graph_builder.add_node("orchestrator", orchestrator_node)
graph_builder.add_node("search_agent", search_agent_node)
graph_builder.add_node("process_results", process_tool_results)

tool_node = ToolNode(tools=search_tools)
graph_builder.add_node("tools", tool_node)

def route_after_orchestrator(state: State) -> str:
    return "search_agent" if state.get("need_search", False) else END

def route_after_search(state: State) -> str:
    last_message = state["messages"][-1]
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    return END

graph_builder.set_entry_point("orchestrator")
graph_builder.add_conditional_edges("orchestrator", route_after_orchestrator)
graph_builder.add_conditional_edges("search_agent", route_after_search)
graph_builder.add_edge("tools", "process_results")
graph_builder.add_edge("process_results", END)

graph = graph_builder.compile(checkpointer=memory)

# ==========================
# ‚ö° OrchestrationAgent async
# ==========================
class OrchestrationAgent:
    """Agent ƒëi·ªÅu ph·ªëi async v·ªõi multi-agent v√† memory gi·ªõi h·∫°n."""

    def __init__(self, thread_id: str = "1"):
        self.graph = graph
        self.config = {"configurable": {"thread_id": thread_id}}

    async def generate_answer(self, message: str):
        try:
            state = {"messages": [HumanMessage(content=message)], "need_search": False}
            result = await self.graph.ainvoke(state, self.config)
            return result["messages"][-1].content
        except Exception as e:
            print(f"‚ùå Chi ti·∫øt l·ªói: {e}")
            import traceback
            traceback.print_exc()
            return f"‚ùå L·ªói agent: {e}"
